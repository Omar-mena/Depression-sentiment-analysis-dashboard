{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "API_KEY = '8xvUEF0lAb858eEnGWu0eYH5j'\n",
    "API_SECRET_KEY = 'sKb1mc2aLkdb2QxFpEXU6VHWiUfOCOUKy9BMJSzkF2abMpjQP3'\n",
    "ACCESS_TOEKN = '1317910496142569475-CLZoQdkMmOMpH6Xzv3B3SDIxOii8bz'\n",
    "ACCESS_TOKEN_SECRET = 'xFscaufPjGdMG7Tq555Kp6HwoFDCaZHXJnNWcwKbpDWmf'\n",
    "auth  = tweepy.OAuthHandler(API_KEY, \\\n",
    "                            API_SECRET_KEY)\n",
    "auth.set_access_token(ACCESS_TOEKN,  \\\n",
    "                      ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK_WORDS = ['depression', 'games']\n",
    "TABLE_NAME = \"Mental_Health\"\n",
    "TABLE_ATTRIBUTES = \"id_str VARCHAR(255), created_at DATETIME, text VARCHAR(255), \\\n",
    "            depression VARCHAR(255), user_created_at VARCHAR(255), user_location VARCHAR(255), \\\n",
    "            user_description VARCHAR(255), user_followers_count INT, longitude DOUBLE, latitude DOUBLE, \\\n",
    "            retweet_count INT, favorite_count INT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "FullData = pd.read_csv(r'C:\\Users\\eagle\\Desktop\\trainingdata\\CombinedTweetsTraining.csv')\n",
    "\n",
    "TrainingData, ValidtingData = train_test_split(FullData, test_size=0.3)\n",
    "\n",
    "TestingData = pd.read_csv(r'C:\\Users\\eagle\\Desktop\\trainingdata\\test1000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\eagle\\anaconda3\\lib\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\eagle\\anaconda3\\lib\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\eagle\\anaconda3\\lib\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\eagle\\anaconda3\\lib\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re, string\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "stopwordEn = stopwords.words('english')\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def tokenization(word):\n",
    "  word = nltk.word_tokenize(word)\n",
    "  return word\n",
    "\n",
    "\n",
    "def lemmatize_sentence_And_POStag(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    porter = PorterStemmer()\n",
    "    stem_tokens = porter.stem(tokens) \n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        elif tag.startswith('ADJ'):\n",
    "            pos = 'j'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word1 = re.sub(r'[^\\w\\s]', '', word)\n",
    "        new_word = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b|@\\w+|#', '', new_word1, flags=re.MULTILINE)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def normalize(words):\n",
    "    words = tokenization(words)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "text_clf = Pipeline([\n",
    "               \n",
    "    ('vect', CountVectorizer(analyzer=normalize and lemmatize_sentence_And_POStag)), # data preproccessing  \n",
    "\n",
    "\n",
    "    \n",
    "    ('clf', SGDClassifier()) # suport vector machine model\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 CountVectorizer(analyzer=<function lemmatize_sentence_And_POStag at 0x0000017577CC4790>)),\n",
       "                ('clf', SGDClassifier())])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(FullData.Text, FullData.Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['depression_model.joblib']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(text_clf, 'depression_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative' 'positive' 'positive' ... 'negative' 'negative' 'negative']\n"
     ]
    }
   ],
   "source": [
    "predicted = text_clf.predict(TestingData.text)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.93152064451158\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.85      0.86      1661\n",
      "    positive       0.27      0.27      0.27       325\n",
      "\n",
      "    accuracy                           0.76      1986\n",
      "   macro avg       0.56      0.56      0.56      1986\n",
      "weighted avg       0.76      0.76      0.76      1986\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>1420</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>237</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          negative  positive\n",
       "negative      1420       241\n",
       "positive       237        88"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(TestingData.Sentiment, predicted)*100) # give the accuracy in percentage \n",
    "\n",
    "target_names = ['negative', 'positive'] # create a confusion matrix table with negative and positive only\n",
    "\n",
    "print(metrics.classification_report(TestingData.Sentiment, predicted, target_names=target_names)) # comparing the real sentimet values with the predicted values \n",
    "\n",
    "# confusion matrix\n",
    "pd.DataFrame(metrics.confusion_matrix(TestingData.Sentiment, predicted),\n",
    "             columns=target_names,index=target_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
